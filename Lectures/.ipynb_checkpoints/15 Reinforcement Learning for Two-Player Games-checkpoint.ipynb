{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\Rv}{\\mathbf{R}}\n",
    "\\newcommand{\\rv}{\\mathbf{r}}\n",
    "\\newcommand{\\Qv}{\\mathbf{Q}}\n",
    "\\newcommand{\\Qnv}{\\mathbf{Qn}}\n",
    "\\newcommand{\\Av}{\\mathbf{A}}\n",
    "\\newcommand{\\Aiv}{\\mathbf{Ai}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\Uv}{\\mathbf{U}}\n",
    "\\newcommand{\\uv}{\\mathbf{u}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\TDv}{\\mathbf{TD}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\Gv}{\\mathbf{G}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\betav_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Two-Player Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Tic-Tac-Toe differ from the maze problem?\n",
    "\n",
    "   * Different state and action sets.\n",
    "   * Two players rather than one.\n",
    "   * Reinforcement is 0 until end of game, when it is 1 for win, 0 for draw, or -1 for loss.\n",
    "   * Maximizing sum of reinforcement rather than minimizing.\n",
    "   * Anything else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the Q Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is the board configuration.  There are $3^9$ of them, though\n",
    "not all are reachable. Is this too big? \n",
    "\n",
    "It is a bit less than 20,000.  Not bad. Is this the full size of the Q table?\n",
    "\n",
    "No. We must add the action dimension.  There are at most 9 actions,\n",
    "one for each cell on the board.  So the Q table will contain about\n",
    "$20,000 \\cdot 9$ values or about 200,000. No worries. \n",
    "\n",
    "Instead of thinking about the Q table as a three-dimensional array, as\n",
    "we did last time, let's be more pythonic and use a dictionary.  Use\n",
    "the current state as the key, and the value associated with the state\n",
    "is an array of Q values for each action taken in that state.\n",
    "\n",
    "We still need a way to represent a board.  \n",
    "\n",
    "How about an array of characters?  So\n",
    "\n",
    "     X |   | O\n",
    "     ---------\n",
    "       | X | O\n",
    "     ---------\n",
    "     X |   |\n",
    "\n",
    "would be\n",
    "\n",
    "     board = np.array(['X',' ','O', ' ','X','O', 'X',' ',' '])\n",
    "\n",
    "The initial board would be\n",
    "\n",
    "     board = np.array([' ']*9)\n",
    "\n",
    "We can represent a move as an index, 0 to 8,  into this array.\n",
    "\n",
    "What should the reinforcement values be?  \n",
    "\n",
    "How about 0 every move except when X wins, with a reinforcement of 1,\n",
    "and when O wins, with a reinforcement of -1.\n",
    "\n",
    "For the above board, let's say we, meaning Player X, prefer move to\n",
    "index 3. In fact, this always results in a win.  So the Q value for\n",
    "move to 3 should be 1.  What other Q values do you know?\n",
    "\n",
    "If we don't play a move to win, O could win in one move.  So the other\n",
    "moves might have Q values close to -1, depending on the skill of\n",
    "Player O.  In the following discussion we will be using a random\n",
    "player for O, so the Q value for a move other than 8 or 3 will be\n",
    "close to but not exactly -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent-World Interaction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our agent to interact with its world, we must implement\n",
    "\n",
    "   1. Initialize Q.\n",
    "   1. Set initial state, as empty board.\n",
    "   1. Repeat:\n",
    "     1. Agent chooses next X move.\n",
    "     1. If X wins, set Q(board,move) to 1.\n",
    "     1. Else, if board is full, set Q(board,move) to 0.\n",
    "     1. Else, let O take move.\n",
    "     1. If O won, update Q(board,move) by (-1 - Q(board,move))\n",
    "     1. For all cases, update Q(oldboard,oldmove) by Q(board,move) - Q(oldboard,oldmove)\n",
    "     1. Shift current board and move to old ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to print a board in the usual Tic-Tac-Toe style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printBoard(board):\n",
    "    print('{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}'.format(*board))\n",
    "\n",
    "board = np.array(['X',' ','O', ' ','X','O', 'X',' ',' '])\n",
    "printBoard(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that returns *True* if the current board is a winning board for us.  We will be Player X.  What does the value of *combos* represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def winner(board):\n",
    "    combos = np.array((0,1,2, 3,4,5, 6,7,8, 0,3,6, 1,4,7, 2,5,8, 0,4,8, 2,4,6))\n",
    "    return np.any(np.logical_or(np.all('X' == board[combos].reshape((-1,3)), axis=1),\n",
    "                                np.all('O' == board[combos].reshape((-1,3)), axis=1)))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = np.array(['X',' ','O', ' ','X','O', 'X',' ',' '])\n",
    "printBoard(board)\n",
    "winner(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[3] = 'X'\n",
    "printBoard(board)\n",
    "winner(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find all valid moves from a board?  Just find all of the spaces in the board representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(board == ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(board == ' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how do we pick one at random and make that move?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = np.array(['X',' ','O', ' ','X','O', 'X',' ',' '])\n",
    "validMoves = np.where(board == ' ')[0]\n",
    "move = np.random.choice(validMoves)\n",
    "boardNew = copy(board)\n",
    "boardNew[move] = 'X'\n",
    "print('From this board')\n",
    "printBoard(board)\n",
    "print('\\n  Move',move)\n",
    "print('\\nresults in board')\n",
    "printBoard(boardNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X just won, we want to set the Q value for the previous state (board) to 1, because X will always win from that state and that action (move).\n",
    "\n",
    "First we must figure out how to implement the Q table.  We want to associate a value with each board and move.  We can use a python dictionary for this.  We know how to represent a board.  A move can be an integer from 0 to 8 to index into the board array for the location to place a marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = {}  # empty table\n",
    "Q[(tuple(board),1)] = 0\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[(tuple(board),1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to look up a Q value for a state,action we have not encountered yet?  It will not be in the dictionary.  We can use the *get* method for the dictionary, that has a second argument as the value returned if the key does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[1] = 'X'\n",
    "Q[(tuple(board),1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.get((tuple(board),1), 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set the Q value for (board,move) to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[(tuple(board),move)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the board is full and we have a draw, then the previous state and action should be assigned 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[(tuple(board),move)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the board is not full, better check to see if O just won.  If O did just win, then we should adjust the Q value of the previous state and X action to be closer to -1, because we just received a -1 reinforcement and the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rho = 0.1 # learning rate\n",
    "Q[(tuple(board),move)] += rho * (-1 - Q[(tuple(board),move)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nobody won yet, let's calculate the temporal difference error and use it to adjust the Q value of the previous board,move. We do this only if we are not at the first move of a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "if step > 0:\n",
    "    Q[(tuple(boardOld),moveOld)] += rho * (Q[(tuple(board),move)] - Q[(tuple(boardOld),moveOld)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, taking random moves is a good strategy, because we know nothing about how to play Tic-Tac-Toe.  But, once we have gained some experience and our Q table has acquired some good predictions of the sum of future reinforcement, we should rely on our Q values to pick good moves.  For a given board, which move is predicted to lead to the best possible future using the current Q table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validMoves = np.where(board == ' ')[0]\n",
    "print('Valid moves are',validMoves)\n",
    "Qs = np.array([Q.get((tuple(board),m), 0) for m in validMoves]) \n",
    "print('Q values for validMoves are',Qs)\n",
    "bestMove = validMoves[np.argmax(Qs)]\n",
    "print('Best move is',bestMove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To slowly transition from taking random actions to taking the action currently believed to be best, called the *greedy* action, we slowly decay a parameter, $\\epsilon$, from 1 down towards 0 as the probability of selecting a random action.  This is called the $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilonGreedy(epsilon, Q, board):\n",
    "    validMoves = np.where(board == ' ')[0]\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        return np.random.choice(validMoves)\n",
    "    else:\n",
    "        # Greedy Move\n",
    "        Qs = np.array([Q.get((tuple(board),m), 0) for m in validMoves]) \n",
    "        return validMoves[ np.argmax(Qs) ]\n",
    "    \n",
    "epsilonGreedy(0.8, Q, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function to make plots to show results of some games.  Say the variable *outcomes* is a vector of 1's, 0's, and -1's, for games in which X wins, draws, and loses, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcomes = np.random.choice([-1,0,1],replace=True,size=(1000))\n",
    "outcomes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotOutcomes(outcomes,epsilons,maxGames,nGames):\n",
    "    if nGames==0:\n",
    "        return\n",
    "    nBins = 100\n",
    "    nPer = int(maxGames/nBins)\n",
    "    outcomeRows = outcomes.reshape((-1,nPer))\n",
    "    outcomeRows = outcomeRows[:int(nGames/float(nPer))+1,:]\n",
    "    avgs = np.mean(outcomeRows,axis=1)\n",
    "    plt.subplot(3,1,1)\n",
    "    xs = np.linspace(nPer,nGames,len(avgs))\n",
    "    plt.plot(xs, avgs)\n",
    "    plt.xlabel('Games')\n",
    "    plt.ylabel('Mean of Outcomes\\n(0=draw, 1=X win, -1=O win)')\n",
    "    plt.title('Bins of {:d} Games'.format(nPer))\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(xs,np.sum(outcomeRows==1,axis=1),'g-',label='Wins')\n",
    "    plt.plot(xs,np.sum(outcomeRows==-1,axis=1),'r-',label='Losses')\n",
    "    plt.plot(xs,np.sum(outcomeRows==0,axis=1),'b-',label='Draws')\n",
    "    plt.legend(loc=\"center\")\n",
    "    plt.ylabel('Number of Games\\nin Bins of {:d}'.format(nPer))\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(epsilons[:nGames])\n",
    "    plt.ylabel('$\\epsilon$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plotOutcomes(outcomes,np.zeros(1000),1000,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's write the whole Tic-Tac-Toe learning loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxGames = 50000\n",
    "rho = 0.2\n",
    "epsilonDecayRate = 0.9999\n",
    "epsilon = 1.0\n",
    "graphics = True\n",
    "showMoves = not graphics\n",
    "\n",
    "outcomes = np.zeros(maxGames)\n",
    "epsilons = np.zeros(maxGames)\n",
    "Q = {}\n",
    "\n",
    "if graphics:\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for nGames in range(maxGames):\n",
    "    epsilon *= epsilonDecayRate\n",
    "    epsilons[nGames] = epsilon\n",
    "    step = 0\n",
    "    board = np.array([' '] * 9)  # empty board\n",
    "    done = False\n",
    "    \n",
    "    while not done:        \n",
    "        step += 1\n",
    "        \n",
    "        # X's turn\n",
    "        move = epsilonGreedy(epsilon, Q, board)\n",
    "        boardNew = copy(board)\n",
    "        boardNew[move] = 'X'\n",
    "        if (tuple(board),move) not in Q:\n",
    "            Q[(tuple(board),move)] = 0  # initial Q value for new board,move\n",
    "        if showMoves:\n",
    "            printBoard(boardNew)\n",
    "            \n",
    "        if winner(boardNew):\n",
    "            # X won!\n",
    "            if showMoves:\n",
    "                print('        X Won!')\n",
    "            Q[(tuple(board),move)] = 1\n",
    "            done = True\n",
    "            outcomes[nGames] = 1\n",
    "            \n",
    "        elif not np.any(boardNew == ' '):\n",
    "            # Game over. No winner.\n",
    "            if showMoves:\n",
    "                print('        draw.')\n",
    "            Q[(tuple(board),move)] = 0\n",
    "            done = True\n",
    "            outcomes[nGames] = 0\n",
    "            \n",
    "        else:\n",
    "            # O's turn.  O is a random player!\n",
    "            moveO = np.random.choice(np.where(boardNew==' ')[0])\n",
    "            boardNew[moveO] = 'O'\n",
    "            if showMoves:\n",
    "                printBoard(boardNew)\n",
    "            if winner(boardNew):\n",
    "                # O won!\n",
    "                if showMoves:\n",
    "                    print('        O Won!')\n",
    "                Q[(tuple(board),move)] += rho * (-1 - Q[(tuple(board),move)])\n",
    "                done = True\n",
    "                outcomes[nGames] = -1\n",
    "        \n",
    "        if step > 1:\n",
    "            Q[(tuple(boardOld),moveOld)] += rho * (Q[(tuple(board),move)] - Q[(tuple(boardOld),moveOld)])\n",
    "            \n",
    "        boardOld, moveOld = board, move # remember board and move to Q(board,move) can be updated after next steps\n",
    "        board = boardNew\n",
    "        \n",
    "        if graphics and (nGames % (maxGames/10) == 0 or nGames == maxGames-1):\n",
    "            fig.clf() \n",
    "            plotOutcomes(outcomes,epsilons,maxGames,nGames-1)\n",
    "            clear_output(wait=True)\n",
    "            display(fig);\n",
    "\n",
    "if graphics:\n",
    "    clear_output(wait=True)\n",
    "print('Outcomes: {:d} X wins {:d} O wins {:d} draws'.format(np.sum(outcomes==1), np.sum(outcomes==-1), np.sum(outcomes==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we examine the Q function that predicts the future for every board and move?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[(tuple([' ']*9),0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[(tuple([' ']*9),1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.get((tuple([' ']*9),0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Q.get((tuple([' ']*9),m), 0) for m in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = np.array([' ']*9)\n",
    "Qs = [Q.get((tuple(board),m), 0) for m in range(9)]\n",
    "printBoard(board)\n",
    "print()\n",
    "print('''{:5.2f} | {:5.2f} | {:5.2f}\n",
    "---------------------\n",
    "{:5.2f} | {:5.2f} | {:5.2f}\n",
    "---------------------\n",
    "{:5.2f} | {:5.2f} | {:5.2f}'''.format(*Qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printBoardQs(board,Q):\n",
    "    printBoard(board)\n",
    "    Qs = [Q.get((tuple(board),m), 0) for m in range(9)]\n",
    "    print()\n",
    "    print('''{:5.2f} | {:5.2f} | {:5.2f}\n",
    "---------------------\n",
    "{:5.2f} | {:5.2f} | {:5.2f}\n",
    "---------------------\n",
    "{:5.2f} | {:5.2f} | {:5.2f}'''.format(*Qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[0] = 'X'\n",
    "board[1] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[4] = 'X'\n",
    "board[3] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[0] = 'X'\n",
    "board[4] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[2] = 'X'\n",
    "board[1] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[7] = 'X'\n",
    "board[3] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board[5] = 'X'\n",
    "board[6] = 'O'\n",
    "printBoardQs(board,Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
